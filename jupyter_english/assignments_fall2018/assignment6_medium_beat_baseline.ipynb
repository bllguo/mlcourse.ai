{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Edited by Sergey Kolchenko (@KolchenkoSergey). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6\n",
    "### <center> Beating baselines in \"How good is your Medium article?\"\n",
    "    \n",
    "<img src='../../img/medium_claps.jpg' width=40% />\n",
    "\n",
    "\n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"A6 baseline\" (~1.45 Public LB score). Do not forget about our shared [\"primitive\" baseline](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline) - you'll find something valuable there.\n",
    "\n",
    "**Your task:**\n",
    " 1. \"Freeride\". Come up with good features to beat the baseline \"A6 baseline\" (for now, public LB is only considered)\n",
    " 2. You need to name your [team](https://www.kaggle.com/c/how-good-is-your-medium-article/team) (out of 1 person) in full accordance with the [course rating](https://drive.google.com/open?id=19AGEhUQUol6_kNLKSzBsjcGUU3qWy3BNUg8x8IFkO3Q). You can think of it as a part of the assignment. 16 credits for beating the mentioned baseline and correct team naming.\n",
    " \n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a6__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack, vstack, save_npz, load_npz\n",
    "from sklearn.linear_model import Ridge, RidgeCV, SGDRegressor\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, TimeSeriesSplit\n",
    "import gc\n",
    "import warnings\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.debugger import set_trace\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from langdetect import detect\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path_to_data):\n",
    "    \n",
    "    content_list = [] \n",
    "    published_list = [] \n",
    "    title_list = []\n",
    "    author_list = []\n",
    "    domain_list = []\n",
    "    tags_list = []\n",
    "    url_list = []\n",
    "    images_list = []\n",
    "    frames_list = []\n",
    "    lang_list = []\n",
    "    \n",
    "    with open(path_to_data, encoding='utf-8') as inp_json_file:\n",
    "        for line in inp_json_file:\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            content_list.append(content_no_html_tags)\n",
    "            published = json_data['published']['$date']\n",
    "            published_list.append(published) \n",
    "            title = json_data['meta_tags']['title'].split('\\u2013')[0].strip() #'Medium Terms of Service – Medium Policy – Medium'\n",
    "            title_list.append(title) \n",
    "            author = json_data['meta_tags']['author'].strip()\n",
    "            author_list.append(author) \n",
    "            domain = json_data['domain']\n",
    "            domain_list.append(domain)\n",
    "            url = json_data['url']\n",
    "            url_list.append(url)\n",
    "            \n",
    "            tags_str = []\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            try:\n",
    "                tag_block = soup.find('ul', class_='tags')\n",
    "                tags = tag_block.find_all('a')\n",
    "                for tag in tags:\n",
    "                    tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n",
    "                tags = ' '.join(tags_str)\n",
    "            except Exception:\n",
    "                tags = 'None'\n",
    "            tags_list.append(tags)\n",
    "            \n",
    "            frames = re.findall(r'iframeContainer', content)\n",
    "            images = re.findall(r'<img class=', content)\n",
    "\n",
    "            images_list.append(len(images))\n",
    "            frames_list.append(len(frames))\n",
    "            try:\n",
    "                lang = detect(content_no_html_tags)\n",
    "            except Exception:      \n",
    "                lang = 'undefined'   \n",
    "            lang_list.append(lang)\n",
    "            \n",
    "    return content_list, published_list, title_list, author_list, domain_list, tags_list, url_list, images_list, frames_list, lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../../data/kaggle_medium' # modify this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_list, published_list, title_list, author_list, domain_list, tags_list, url_list, images_list, frames_list, lang_list = extract_features(os.path.join(PATH_TO_DATA, 'train.json'))\n",
    "train = pd.DataFrame()\n",
    "train['content'] = content_list\n",
    "train['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "train['title'] = title_list\n",
    "train['author'] = author_list\n",
    "train['domain'] = domain_list\n",
    "train['tags'] = tags_list\n",
    "train['length'] = train['content'].apply(len)\n",
    "train['url'] = url_list\n",
    "train['images'] = images_list\n",
    "train['frames'] = frames_list\n",
    "train['lang'] = lang_list\n",
    "\n",
    "content_list, published_list, title_list, author_list, domain_list, tags_list, url_list, images_list, frames_list, lang_list = extract_features(os.path.join(PATH_TO_DATA, 'test.json'))\n",
    "test = pd.DataFrame()\n",
    "test['content'] = content_list\n",
    "test['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "test['title'] = title_list\n",
    "test['author'] = author_list\n",
    "test['domain'] = domain_list\n",
    "test['tags'] = tags_list\n",
    "test['length'] = test['content'].apply(len)\n",
    "test['url'] = url_list\n",
    "test['images'] = images_list\n",
    "test['frames'] = frames_list\n",
    "test['lang'] = lang_list\n",
    "del content_list, published_list, title_list, author_list, domain_list, tags_list, url_list, images_list, frames_list, lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_pickle('train.pkl')\n",
    "#test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test])\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Building datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_split = y_train.shape[0]\n",
    "idx_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(max_features=50000, min_df = 0.1, max_df = 0.8)\n",
    "sparse_train = cv.fit_transform(train['content'])\n",
    "sparse_test  = cv.transform(test['content'])\n",
    "full_sparse_data =  vstack([sparse_train, sparse_test])\n",
    "corpus_data_gensim = gensim.matutils.Sparse2Corpus(full_sparse_data, documents_columns=False)\n",
    "del sparse_train, sparse_test, full_sparse_data\n",
    "\n",
    "lda = gensim.models.LdaModel(corpus_data_gensim, num_topics = 30)\n",
    "\n",
    "def document_to_lda_features(lda_model, document):\n",
    "    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n",
    "    topic_importances = np.array(topic_importances)\n",
    "    return topic_importances[:,1]\n",
    "\n",
    "lda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))\n",
    "data_pd_lda_features = pd.DataFrame(lda_features)\n",
    "\n",
    "df_lda_train = data_pd_lda_features.iloc[:idx_split, :]\n",
    "df_lda_test = data_pd_lda_features.iloc[idx_split:, :]\n",
    "del data_pd_lda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, df_lda_train], axis=1)\n",
    "test = pd.concat([test, df_lda_test.reset_index(drop=True)], axis=1)\n",
    "del df_lda_train, df_lda_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>tags</th>\n",
       "      <th>length</th>\n",
       "      <th>url</th>\n",
       "      <th>images</th>\n",
       "      <th>frames</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Susan BrattonTrusted Hot Sex Advisor To Millio...</td>\n",
       "      <td>1970-01-01 00:00:00.001</td>\n",
       "      <td>Saving Your Marriage By Watching Steamy Sex Ed...</td>\n",
       "      <td>Susan Bratton</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>Lovemaking Sex SexPositions EarlyBird SexEdVideos</td>\n",
       "      <td>5473</td>\n",
       "      <td>http://personallifemedia.com/2017/01/saving-ma...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.147627</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryo OoishiDec 31, 1969やってよかった中学受験明日から息子の中学受験がは...</td>\n",
       "      <td>1970-01-01 00:00:00.001</td>\n",
       "      <td>やってよかった中学受験</td>\n",
       "      <td>Ryo Ooishi</td>\n",
       "      <td>medium.com</td>\n",
       "      <td></td>\n",
       "      <td>5325</td>\n",
       "      <td>https://medium.com/@ooishi/%E3%82%84%E3%81%A3%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>なぞちゅう仮面ライダーとかスーパー戦隊を愛する30代。特撮はたしなむ程度（自称）色々なもの、...</td>\n",
       "      <td>1970-01-18 03:21:32.400</td>\n",
       "      <td>はてなブログに書いた今年の手帳のお話</td>\n",
       "      <td>なぞちゅう</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>徒然日記 手帳 ブログ</td>\n",
       "      <td>2487</td>\n",
       "      <td>http://nazoblackrx.hatenablog.com/entry/2016/1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Internet Corporation LLCDec 8, 1987SPECIAL NOT...</td>\n",
       "      <td>1987-12-08 21:45:00.000</td>\n",
       "      <td>Internet Corporation LLC to Acquire Early Clue...</td>\n",
       "      <td>Internet Corporation LLC</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>SocialMedia EarlyClues InternetCorporationLlc</td>\n",
       "      <td>11285</td>\n",
       "      <td>https://medium.com/the-internet-corporation/de...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431693</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.019082</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.304391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mackenzie OldridgeDec 29, 2003g sowtwaretradin...</td>\n",
       "      <td>2003-12-29 17:00:00.000</td>\n",
       "      <td>g sowtwaretrading botMoneyMoneyMakeGetting To ...</td>\n",
       "      <td>Mackenzie Oldridge</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>Finance Trading</td>\n",
       "      <td>12541</td>\n",
       "      <td>http://www.investopedia.com/articles/optioninv...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452453</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.310134</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               published  \\\n",
       "0  Susan BrattonTrusted Hot Sex Advisor To Millio... 1970-01-01 00:00:00.001   \n",
       "1  Ryo OoishiDec 31, 1969やってよかった中学受験明日から息子の中学受験がは... 1970-01-01 00:00:00.001   \n",
       "2  なぞちゅう仮面ライダーとかスーパー戦隊を愛する30代。特撮はたしなむ程度（自称）色々なもの、... 1970-01-18 03:21:32.400   \n",
       "3  Internet Corporation LLCDec 8, 1987SPECIAL NOT... 1987-12-08 21:45:00.000   \n",
       "4  Mackenzie OldridgeDec 29, 2003g sowtwaretradin... 2003-12-29 17:00:00.000   \n",
       "\n",
       "                                               title  \\\n",
       "0  Saving Your Marriage By Watching Steamy Sex Ed...   \n",
       "1                                        やってよかった中学受験   \n",
       "2                                 はてなブログに書いた今年の手帳のお話   \n",
       "3  Internet Corporation LLC to Acquire Early Clue...   \n",
       "4  g sowtwaretrading botMoneyMoneyMakeGetting To ...   \n",
       "\n",
       "                     author      domain  \\\n",
       "0             Susan Bratton  medium.com   \n",
       "1                Ryo Ooishi  medium.com   \n",
       "2                     なぞちゅう  medium.com   \n",
       "3  Internet Corporation LLC  medium.com   \n",
       "4        Mackenzie Oldridge  medium.com   \n",
       "\n",
       "                                                tags  length  \\\n",
       "0  Lovemaking Sex SexPositions EarlyBird SexEdVideos    5473   \n",
       "1                                                       5325   \n",
       "2                                        徒然日記 手帳 ブログ    2487   \n",
       "3      SocialMedia EarlyClues InternetCorporationLlc   11285   \n",
       "4                                    Finance Trading   12541   \n",
       "\n",
       "                                                 url  images  frames  \\\n",
       "0  http://personallifemedia.com/2017/01/saving-ma...       6       0   \n",
       "1  https://medium.com/@ooishi/%E3%82%84%E3%81%A3%...       0       0   \n",
       "2  http://nazoblackrx.hatenablog.com/entry/2016/1...       1       0   \n",
       "3  https://medium.com/the-internet-corporation/de...      13       0   \n",
       "4  http://www.investopedia.com/articles/optioninv...       2       0   \n",
       "\n",
       "     ...           20        21        22        23        24        25  \\\n",
       "0    ...     0.000090  0.000090  0.028759  0.000090  0.147627  0.000090   \n",
       "1    ...     0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "2    ...     0.003333  0.003333  0.003333  0.003333  0.003333  0.003333   \n",
       "3    ...     0.431693  0.000072  0.019082  0.000072  0.000072  0.000072   \n",
       "4    ...     0.452453  0.000048  0.000048  0.000048  0.000048  0.000048   \n",
       "\n",
       "         26        27        28        29  \n",
       "0  0.000090  0.000090  0.000090  0.000090  \n",
       "1  0.011111  0.011111  0.011111  0.011111  \n",
       "2  0.003333  0.003333  0.003333  0.003333  \n",
       "3  0.000072  0.000072  0.000072  0.304391  \n",
       "4  0.310134  0.000048  0.000048  0.000048  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'] = y_train\n",
    "train.sort_values('published', inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "y_train = train['target'].values\n",
    "train.drop('target', axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer_content = TfidfVectorizer(ngram_range=(1, 2), max_features = 100000, stop_words='english')\n",
    "vectorizer_title = TfidfVectorizer(ngram_range=(1, 2), max_features = 100000, stop_words='english')\n",
    "vectorizer_tags = TfidfVectorizer(max_features = 10000, stop_words='english')\n",
    "content_train = vectorizer_content.fit_transform(train['content'])\n",
    "title_train = vectorizer_title.fit_transform(train['title'])\n",
    "tags_train = vectorizer_tags.fit_transform(train['tags'])\n",
    "content_test = vectorizer_content.transform(test['content'])\n",
    "title_test = vectorizer_title.transform(test['title'])\n",
    "tags_test = vectorizer_tags.transform(test['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectorizer_content, vectorizer_title, vectorizer_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test])\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['published'].apply(lambda x: x.year)\n",
    "df['weekend'] = df['published'].apply(lambda x: x.dayofweek >= 5)\n",
    "df['weekend'].replace({True : 1, False : 0}, inplace=True)\n",
    "df['dow_sin'] = df['published'].apply(lambda x: np.sin(2*np.pi*x.dayofweek/7))\n",
    "df['dow_cos'] = df['published'].apply(lambda x: np.cos(2*np.pi*x.dayofweek/7))\n",
    "df['month_sin'] = df['published'].apply(lambda x: np.sin(2*np.pi*x.month/12))\n",
    "df['month_cos'] = df['published'].apply(lambda x: np.cos(2*np.pi*x.month/12))\n",
    "df['hour_sin'] = df['published'].apply(lambda x: np.sin(2*np.pi*x.hour/24))\n",
    "df['hour_cos'] = df['published'].apply(lambda x: np.cos(2*np.pi*x.hour/24))\n",
    "df['hour'] = df['published'].apply(lambda x: x.hour)\n",
    "df['morning'] = ((df['hour'] >= 7) & (df['hour'] <= 11)).astype('int')\n",
    "df['afternoon'] = ((df['hour'] >= 12) & (df['hour'] <= 18)).astype('int')\n",
    "df['evening'] = ((df['hour'] >= 19) & (df['hour'] <= 23)).astype('int')\n",
    "df['night'] = ((df['hour'] >= 0) & (df['hour'] <= 6)).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc. features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>tags</th>\n",
       "      <th>length</th>\n",
       "      <th>url</th>\n",
       "      <th>images</th>\n",
       "      <th>frames</th>\n",
       "      <th>...</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>hour</th>\n",
       "      <th>morning</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>number_of_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Susan BrattonTrusted Hot Sex Advisor To Millio...</td>\n",
       "      <td>1970-01-01 00:00:00.001</td>\n",
       "      <td>Saving Your Marriage By Watching Steamy Sex Ed...</td>\n",
       "      <td>Susan Bratton</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>Lovemaking Sex SexPositions EarlyBird SexEdVideos</td>\n",
       "      <td>5473</td>\n",
       "      <td>http://personallifemedia.com/2017/01/saving-ma...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryo OoishiDec 31, 1969やってよかった中学受験明日から息子の中学受験がは...</td>\n",
       "      <td>1970-01-01 00:00:00.001</td>\n",
       "      <td>やってよかった中学受験</td>\n",
       "      <td>other</td>\n",
       "      <td>medium.com</td>\n",
       "      <td></td>\n",
       "      <td>5325</td>\n",
       "      <td>https://medium.com/@ooishi/%E3%82%84%E3%81%A3%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>なぞちゅう仮面ライダーとかスーパー戦隊を愛する30代。特撮はたしなむ程度（自称）色々なもの、...</td>\n",
       "      <td>1970-01-18 03:21:32.400</td>\n",
       "      <td>はてなブログに書いた今年の手帳のお話</td>\n",
       "      <td>other</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>徒然日記 手帳 ブログ</td>\n",
       "      <td>2487</td>\n",
       "      <td>http://nazoblackrx.hatenablog.com/entry/2016/1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Internet Corporation LLCDec 8, 1987SPECIAL NOT...</td>\n",
       "      <td>1987-12-08 21:45:00.000</td>\n",
       "      <td>Internet Corporation LLC to Acquire Early Clue...</td>\n",
       "      <td>other</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>SocialMedia EarlyClues InternetCorporationLlc</td>\n",
       "      <td>11285</td>\n",
       "      <td>https://medium.com/the-internet-corporation/de...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mackenzie OldridgeDec 29, 2003g sowtwaretradin...</td>\n",
       "      <td>2003-12-29 17:00:00.000</td>\n",
       "      <td>g sowtwaretrading botMoneyMoneyMakeGetting To ...</td>\n",
       "      <td>other</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>Finance Trading</td>\n",
       "      <td>12541</td>\n",
       "      <td>http://www.investopedia.com/articles/optioninv...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content               published  \\\n",
       "0  Susan BrattonTrusted Hot Sex Advisor To Millio... 1970-01-01 00:00:00.001   \n",
       "1  Ryo OoishiDec 31, 1969やってよかった中学受験明日から息子の中学受験がは... 1970-01-01 00:00:00.001   \n",
       "2  なぞちゅう仮面ライダーとかスーパー戦隊を愛する30代。特撮はたしなむ程度（自称）色々なもの、... 1970-01-18 03:21:32.400   \n",
       "3  Internet Corporation LLCDec 8, 1987SPECIAL NOT... 1987-12-08 21:45:00.000   \n",
       "4  Mackenzie OldridgeDec 29, 2003g sowtwaretradin... 2003-12-29 17:00:00.000   \n",
       "\n",
       "                                               title         author  \\\n",
       "0  Saving Your Marriage By Watching Steamy Sex Ed...  Susan Bratton   \n",
       "1                                        やってよかった中学受験          other   \n",
       "2                                 はてなブログに書いた今年の手帳のお話          other   \n",
       "3  Internet Corporation LLC to Acquire Early Clue...          other   \n",
       "4  g sowtwaretrading botMoneyMoneyMakeGetting To ...          other   \n",
       "\n",
       "       domain                                               tags  length  \\\n",
       "0  medium.com  Lovemaking Sex SexPositions EarlyBird SexEdVideos    5473   \n",
       "1  medium.com                                                       5325   \n",
       "2  medium.com                                        徒然日記 手帳 ブログ    2487   \n",
       "3  medium.com      SocialMedia EarlyClues InternetCorporationLlc   11285   \n",
       "4  medium.com                                    Finance Trading   12541   \n",
       "\n",
       "                                                 url  images  frames  \\\n",
       "0  http://personallifemedia.com/2017/01/saving-ma...       6       0   \n",
       "1  https://medium.com/@ooishi/%E3%82%84%E3%81%A3%...       0       0   \n",
       "2  http://nazoblackrx.hatenablog.com/entry/2016/1...       1       0   \n",
       "3  https://medium.com/the-internet-corporation/de...      13       0   \n",
       "4  http://www.investopedia.com/articles/optioninv...       2       0   \n",
       "\n",
       "        ...           month_sin  month_cos  hour_sin  hour_cos  hour  morning  \\\n",
       "0       ...        5.000000e-01   0.866025  0.000000  1.000000     0        0   \n",
       "1       ...        5.000000e-01   0.866025  0.000000  1.000000     0        0   \n",
       "2       ...        5.000000e-01   0.866025  0.707107  0.707107     3        0   \n",
       "3       ...       -2.449294e-16   1.000000 -0.707107  0.707107    21        0   \n",
       "4       ...       -2.449294e-16   1.000000 -0.965926 -0.258819    17        0   \n",
       "\n",
       "   afternoon  evening  night  number_of_tags  \n",
       "0          0        0      1               5  \n",
       "1          0        0      1               0  \n",
       "2          0        0      1               3  \n",
       "3          0        1      0               3  \n",
       "4          1        0      0               2  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df['author'].value_counts()\n",
    "df['author'] = df['author'].replace(counts[counts <= 5].index.values, 'other')\n",
    "df['number_of_tags'] = df['tags'].apply(lambda x: len(x.split()))\n",
    "counts = df['domain'].value_counts()\n",
    "df['domain'] = df['domain'].replace(counts[counts <= 100].index.values, 'other')\n",
    "del counts\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['content', 'title', 'tags', 'length', 'published', 'hour', 'url'], axis=1, inplace=True)\n",
    "df_dummies = pd.get_dummies(df, columns = ['author', 'domain', 'images', 'frames', 'lang', 'year', 'number_of_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = csr_matrix(hstack([df_dummies.iloc[:idx_split,:],\n",
    "                             content_train,\n",
    "                             title_train,\n",
    "                             tags_train]))\n",
    "X_test = csr_matrix(hstack([df_dummies.iloc[idx_split:,:],\n",
    "                             content_test,\n",
    "                             title_test,\n",
    "                             tags_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('train.npz', X_train)\n",
    "save_npz('test.npz', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_npz('train.npz')\n",
    "X_test = load_npz('test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain, X_valid, y_ttrain, y_valid = train_test_split(X_train, y_train, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ridge.fit(X_ttrain, y_ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0742546550289112"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_valid, ridge.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "param_grid = {'alpha':np.logspace(-4, 4, 10)}\n",
    "ridgeSearch = RandomizedSearchCV(ridge, param_grid, scoring='neg_mean_absolute_error', verbose=1, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 27.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=5),\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'alpha': array([  1.00000e-04,   7.74264e-04,   5.99484e-03,   4.64159e-02,\n",
       "         3.59381e-01,   2.78256e+00,   2.15443e+01,   1.66810e+02,\n",
       "         1.29155e+03,   1.00000e+04])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_absolute_error',\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ridgeSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 2.7825594022071258}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgeSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "params = {'bagging_fraction' : [.7, .85, 1],\n",
    "          'bagging_freq' : [0, 25, 50],\n",
    "          'num_leaves' : [31],\n",
    "          'n_estimators' : [400],\n",
    "          'learning_rate' : [.01],\n",
    "          'metric' : ['mean_absolute_error'],\n",
    "          'objective' : ['mean_absolute_error'],\n",
    "          'num_iterations' : [700]}\n",
    "lgb_mdl = lgb.LGBMRegressor()\n",
    "lgbSearch = RandomizedSearchCV(lgb_mdl, params, cv=cv, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 474.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8h 12min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=5),\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "       n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'bagging_fraction': [0.7, 0.85, 1], 'bagging_freq': [0, 25, 50], 'num_leaves': [31], 'n_estimators': [400], 'learning_rate': [0.01], 'metric': ['mean_absolute_error'], 'objective': ['mean_absolute_error'], 'num_iterations': [700]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgbSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.7,\n",
       " 'bagging_freq': 25,\n",
       " 'learning_rate': 0.01,\n",
       " 'metric': 'mean_absolute_error',\n",
       " 'n_estimators': 400,\n",
       " 'num_iterations': 700,\n",
       " 'num_leaves': 31,\n",
       " 'objective': 'mean_absolute_error'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mdl = Ridge(2.7825594022071258)\n",
    "mdl.fit(X_train, y_train)\n",
    "mdl_pred = mdl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_lgb = lgb.LGBMRegressor(**lgbSearch.best_params_)\n",
    "mdl_lgb.fit(X_train, y_train)\n",
    "mdl_lgb_pred = mdl_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred = .5*mdl_pred + .5*mdl_lgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = ensemble_pred + 4.33328 - np.mean(ensemble_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, \n",
    "                                                      'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(final_pred, os.path.join(PATH_TO_DATA,\n",
    "                                                    'assignment6_medium_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now's the time for dirty Kaggle hacks. Form a submission file with all zeros. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(np.zeros_like(ridge_test_pred), \n",
    "                      os.path.join(PATH_TO_DATA,\n",
    "                                   'medium_all_zeros_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred_modif = ridge_test_pred # You code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred_modif, \n",
    "                      os.path.join(PATH_TO_DATA,\n",
    "                                   'assignment6_medium_submission_with_hack.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the assignment. Much more credits will be given to the winners in this competition, check [course roadmap](https://mlcourse.ai/roadmap). Do not spoil the assignment and the competition - don't share high-performing kernels (with MAE < 1.5).\n",
    "\n",
    "Some ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will learn much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- In our course, we don't cover neural nets. But it's not obliged to use GRUs/LSTMs/whatever in this competition.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "<img src='../../img/kaggle_shakeup.png' width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
